{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian feedforward\n",
    "Ro Jefferson<br>\n",
    "Last updated 2021-05-19 \n",
    "\n",
    "This notebook grew out of some explorations of criticality in random neural nets, based primarily on the works by [Schoenholz et al.](https://arxiv.org/abs/1611.01232) and [Poole et al.](https://arxiv.org/abs/1606.05340); see also my [blog article](https://rojefferson.blog/2020/06/19/criticality-in-deep-neural-nets/) for a pedagogical treatment of the underlying idea. The titular \"Gaussian\" refers to the fact that we work with a *random* feedforward neural network here, in which the weights and biases are randomly initialized following some Gaussian distribution(s); in the large-$N$ limit, each layer (as well as the network as a whole) behaves like a Gaussian distribution, which simplifies the analysis considerably. \n",
    "\n",
    "This notebook constructs and trains basic feedforward networks of arbitrary depth on the MNIST database, using the built-in `cross_entropy` as the loss function and $tanh$ for the non-linearity. In particular, it is designed to fascillitate comparing a range of different depths for a given set of hyperparameters---especially the variance of the distrubution of weights and biases, which control the phase (ordered vs. chaotic). The data -- accuracies, hooks, model parameters -- are optionally written as HDF5 files to the specified directory. The data are then deleted from the kernel in order to free sufficient memory for the next model. **The user must specify** the `PATH_TO_MNIST` and the `PATH_TO_DATA` below. \n",
    "\n",
    "Hooks are computationally intensive and are thus disabled by default. One must pass `hooks=True` when calling `train_models()` to record the layer inputs/outputs, in which case they will be stored *only* for the beginning and end of each run (to minimize computation time while allowing before vs. after analysis). Similarly for the parameters (weights, biases), which we may use in another notebook to compute the KL divergence.\n",
    "\n",
    "The companion notebok \"Gaussian_Feedforward_Analysis.ipynb\" is designed to read the aforementioned HDF5 files and perform some analysis, while \"RelativeEntropy_Nonsymbolic.ipynb\" reads them to compute the KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch packages:\n",
    "import torch\n",
    "import torch.nn as nn                       # neural net package\n",
    "import torch.nn.functional as F             # useful functions, e.g., convolutions & loss functions\n",
    "from torch import optim                     # optimizers (torch.optim)\n",
    "from torch.utils.data import TensorDataset  # for wrapping tensors\n",
    "from torch.utils.data import DataLoader     # for managing batches\n",
    "\n",
    "# Numpy, scipy, and plotting:\n",
    "import numpy as np\n",
    "from scipy.stats import norm         # Gaussian fitting\n",
    "import scipy.integrate as integrate  # integration\n",
    "import matplotlib.pyplot as plt      # plotting\n",
    "import seaborn as sns; sns.set()     # nicer plotting\n",
    "import pandas as pd                  # dataframe for use with seaborn\n",
    "\n",
    "# File i/o:\n",
    "import pickle  # for unpickling MNIST data\n",
    "import gzip    # for opening pickled MNIST data file\n",
    "import h5py    # HDF5\n",
    "\n",
    "# Miscellaneous:\n",
    "import math\n",
    "import random  # random number generators\n",
    "import re      # regular expressions\n",
    "import gc      # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory tracking (optional/unused):\n",
    "import os, psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "        \n",
    "# Example usage:\n",
    "#print('RSS = ', process.memory_info().rss/10**6, 'MB')  # resident set size (RAM)\n",
    "#print('VMS = ', process.memory_info().vms/10**6, 'MB')  # virtual memory (RAM + swap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and pre-process MNIST data\n",
    "Since our focus is on the structure/dynamics of the network rather than state-of-the art optimizations, we'll just use the vanilla MNIST dataset for this notebook. We first unzip and unpickle the dataset, and load it into a training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MNIST = '/full/path/to/local/MNIST/gzip/file/'\n",
    "FILENAME = 'mnist.pkl.gz'\n",
    "\n",
    "# open (and automatically close) gzip file in mode for reading binary (`rb`) data:\n",
    "with gzip.open(PATH_TO_MNIST + FILENAME, 'rb') as file:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(file, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optinally, in the case of memory limitations or testing, we can opt to work with a small subset of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truncate = 5000  # max 50,000 training and 10,000 validation images\n",
    "#x_train, y_train, x_valid, y_valid = x_train[:truncate], y_train[:truncate], x_valid[:truncate], y_valid[:truncate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image consists of $28\\times28$ pixels (where each pixel value is a float between 0 and 1), flattened into a row of length 784. Currently however, each image is a numpy array; to use PyTorch, we need to convert this to a `torch.tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(torch.from_numpy, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're on the subject of file i/o, let's choose a location to store any data files we create below (n.b., must end with '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '/full/path/to/desired/write/directory/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to specify whether to create a wide or decimated model (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDE = True   # True avoids normalization issues in separate KL divergence computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the model(s)\n",
    "While PyTorch's built-in `Linear` layer seems to exhibit better performance out-of-the-box, it doesn't quite suffice for our purposes, since it uses a uniform distribution for the weight & bias initialization; so instead, we'll define a custom layer in which the parameters are initialized along a Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear layer z=Wx+b, with W,b drawn from normal distributions:\n",
    "class GaussianLinear(nn.Module):\n",
    "    def __init__(self, size_in, size_out, var_W, var_b):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        self.var_W, self.var_b = var_W/size_in, var_b  # n.b., must scale var_W by layer width!\n",
    "\n",
    "        # normally distributed weights with mean=0 and variance=var_W/N:\n",
    "        norm_vec = torch.normal(mean=torch.zeros(size_out*size_in), std=math.sqrt(self.var_W))\n",
    "        self.weights = nn.Parameter(norm_vec.view(size_out, size_in))\n",
    "        \n",
    "        # normally distributed biases with mean=0 and variance=var_b:\n",
    "        self.bias = nn.Parameter(torch.normal(mean=torch.zeros(size_out), std=math.sqrt(var_b)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        prod = torch.mm(x, self.weights.t())  # Wx\n",
    "        return torch.add(prod, self.bias)     # Wx+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need functions to compute the gradients and update the parameters -- i.e., to train the model -- subject to our choice of loss function. We'll just use the built-in SGD optimizer, with the built-in cross-entropy as our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradients & update parameters for a single batch, given loss function & optimizer:\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)  # compute specified loss function for the model\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()  # compute gradients\n",
    "        opt.step()       # update parameters\n",
    "        opt.zero_grad()  # zero gradients in preparation for next iteration\n",
    "\n",
    "    # n.b., detaching returns the value of the loss; without, returns entire computation graph!\n",
    "    return loss.detach().item(), len(xb)\n",
    "\n",
    "# compute accuracy; predicted digit corresponds to index with maximum value:\n",
    "def accuracy(preds, yb):\n",
    "    preds = torch.argmax(preds, 1)       # max argument along axis 1 (n.b., batch size must be > 1, else error)\n",
    "    return (preds == yb).float().mean()  # for each element: 1 if prediction matches target, 0 otherwise\n",
    "\n",
    "# train & evaluate the model, given loss function, optimizer, and DataLoaders:\n",
    "def fit(epochs, model, depth, hooks, file_hook, write_params, file_params, var_w, var_b, loss_func, opt, train_dl, valid_dl, acc_list=-1, loss_list=-1):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # register hooks only on first and last epoch:\n",
    "        with torch.no_grad():\n",
    "            if hooks and (epoch == 0 or epoch == epochs-1):\n",
    "                inputs, outputs = [], []\n",
    "                hook_layers(model, inputs, outputs)\n",
    "            \n",
    "        model.train()  # ensure training mode (e.g., dropout)\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()   # ensure evaluation mode (e.g., no dropout)\n",
    "        with torch.no_grad():\n",
    "            # compute loss:\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl])  # * unzips\n",
    "            val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            \n",
    "            # compute accuracy:\n",
    "            accuracies = np.array([accuracy(model(xb), yb) for xb, yb in valid_dl])\n",
    "            val_acc = np.sum(accuracies)/len(accuracies)\n",
    "            \n",
    "        print(epoch, val_loss, val_acc)  # monitor progress\n",
    "        \n",
    "        # save progress only if user passed lists (for speed if not):\n",
    "        if isinstance(loss_list, list):\n",
    "            loss_list.append(val_loss)\n",
    "        if isinstance(acc_list, list):\n",
    "            acc_list.append(val_acc)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # optionally write initial & final hooks, parameters:\n",
    "            if epoch == 0 or epoch == epochs-1:\n",
    "                if write_params and isinstance(file_params, str):    # check valid filename\n",
    "                    write_parameters('e{}-'.format(epoch) + file_params, model, depth, var_w, var_b)\n",
    "            \n",
    "                if hooks and isinstance(file_hook, str):    # check valid filename\n",
    "                    write_hooks('e{}-'.format(epoch) + file_hook, depth, inputs, outputs)\n",
    "                    # clear hooks:\n",
    "                    inputs, outputs= -1, -1\n",
    "                    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract intermediate inputs & activations for later analysis, we'll create a functrion that adds forward hooks to the `nn.Tanh` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple class to store layer inputs & outputs:\n",
    "class Hook():\n",
    "    def __init__(self, module, input=None, output=None):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.input = input\n",
    "        #self.output = output\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input.append(input[0].detach())\n",
    "        #self.output.append(output.detach())\n",
    "        \n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "# function that recursively registers hooks on Tanh layers:\n",
    "def hook_layers(net, inputs, outputs):\n",
    "    for name, layer in net._modules.items():\n",
    "        # if nn.Sequential, register recursively on constituent modules:\n",
    "        if isinstance(layer, nn.Sequential):\n",
    "            hook_layers(layer, inputs, outputs)\n",
    "        # individual module, register hook only on Tanh:\n",
    "        elif isinstance(layer, nn.Tanh):\n",
    "            Hook(layer, inputs, outputs)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, it is convenient to use PyTorch's `DataLoader` utility to handle batch management, so we'll define a function to load our training & validation data into that form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return DataLoaders for training and validation sets, for batch management:\n",
    "def get_data(x_train, y_train, x_valid, y_valid, batch_size):\n",
    "    return (DataLoader(TensorDataset(x_train, y_train), batch_size, shuffle=False),\n",
    "            DataLoader(TensorDataset(x_valid, y_valid), batch_size*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to actually build the model (i.e., the network). To fascillitate playing with different depths, let's create a function that constructs a network of arbitrary depth consisting of `GaussianLinear` layers followed by `Tanh` layers, and which steadily reduces the number of neurons per layer in step sizes of (784-10)/num_layers (n.b., \"arbitrary\" up to maximum depth of 774, given monotonic reduction constraint). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************ used only if `WIDE=False` *************************************\n",
    "# Construct a Gaussian neural network consisting of GaussianLinear layers followed by Tanh layers.\n",
    "# The layer widths are steadily reduced from input_dim to output_dim\n",
    "# in step sizes of (input.dim - output.dim)/n_layers (n.b., implies max depth).\n",
    "def build_network(num_layers, input_dim, output_dim, var_w, var_b): \n",
    "    # determine how much to shrink each layer:\n",
    "    diff = input_dim - output_dim   \n",
    "    if num_layers > diff:\n",
    "        raise Exception('Specified number of layers exceeds maximum value consistent\\n'\n",
    "                        'with monotonic reduction in layer widths. Max allowed depth is {}'.format(diff))\n",
    "            \n",
    "    shrink = math.floor(diff/num_layers)  # n.b., rounding up can over-decimate in deep networks!\n",
    "    \n",
    "    # compute layer widths:\n",
    "    widths = []\n",
    "    for i in range(num_layers):\n",
    "        widths.append(input_dim - shrink*i)      \n",
    "    \n",
    "    # output layer:\n",
    "    widths.append(output_dim)\n",
    "    \n",
    "    # construct and add layers to list (no need to use nn.ModuleList):\n",
    "    mlist = []\n",
    "    for i in range(num_layers):\n",
    "        mlist.append(GaussianLinear(widths[i], widths[i+1], var_w, var_b))\n",
    "        mlist.append(nn.Tanh())\n",
    "    \n",
    "    return nn.Sequential(*mlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, to test my hypothesis that pathological behaviour in the KL divergence is due to dimensional reduction (i.e., normalization), we can experiment with constant-width networks (at least up until the very end, where we must shrink down to 10): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************ used only if `WIDE=True` *************************************\n",
    "# Construct a Gaussian neural network consisting of GaussianLinear layers followed by Tanh layers.\n",
    "# Layer widths are kept at 784 until the second-from-last layer, at which point we reduce to\n",
    "# 400, and then 10 in the output layer.\n",
    "def build_wide_network(num_layers, input_dim, output_dim, var_w, var_b): \n",
    "    # check num_layers > 3:\n",
    "    if num_layers < 3:\n",
    "        raise Exception('Too few layers; minimum allowed depth is 3.')\n",
    "    \n",
    "    # compute layer widths:\n",
    "    widths = [input_dim]*(num_layers-1)\n",
    "    widths.append(400)\n",
    "    widths.append(10)\n",
    "\n",
    "    # construct and add layers to list (no need to use nn.ModuleList):\n",
    "    mlist = []\n",
    "    for i in range(num_layers):\n",
    "        mlist.append(GaussianLinear(widths[i], widths[i+1], var_w, var_b))\n",
    "        mlist.append(nn.Tanh())\n",
    "    \n",
    "    return nn.Sequential(*mlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write a function that encapsulates creating and training a list of models of different depths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and train models with a range of depths;\n",
    "# pass -1 (or any non-str) for file names to avoid writing:\n",
    "def train_models(depth_min, depth_max, depth_step=1, file_acc='accuracies.hdf5',\n",
    "                 write_params=False, file_params='parameters.hdf5',\n",
    "                 hooks=False, file_hook='hooks.hdf5', \n",
    "                 save_model=False, file_model='model.hdf5'):\n",
    "    depth = np.arange(depth_min, depth_max, depth_step)\n",
    "    print('Depth list: ', depth)\n",
    "            \n",
    "    # construct new set of models & associated optimizers:\n",
    "    model = []\n",
    "    opt = []\n",
    "    for i,d in enumerate(depth):\n",
    "        if not WIDE:\n",
    "            model.append(build_network(d, 784, 10, var_weight, var_bias))\n",
    "        else:\n",
    "            model.append(build_wide_network(d, 784, 10, var_weight, var_bias))  # alternative: wide network\n",
    "        opt.append(optim.SGD(model[i].parameters(), rate, momentum))\n",
    "                        \n",
    "    # train models, optionally write data:\n",
    "    for i in range(len(model)):\n",
    "        accuracies = []  # store accuracies\n",
    "        \n",
    "        print('\\nTraining model ', i, ' with depth ', depth[i], '...')\n",
    "\n",
    "        fit(epochs, model[i], depth[i], hooks, file_hook, write_params, file_params, var_weight, var_bias, loss_func, opt[i], train_dl, valid_dl, accuracies)\n",
    "        \n",
    "        if save_model:\n",
    "            model_name = PATH_TO_DATA + re.sub('\\.(.*)$','',file_model) + '-{}.hdf5'.format(depth[i])\n",
    "            torch.save(model[i].state_dict(), model_name)\n",
    "        \n",
    "        # optionally write accuracies in hdf5 format:\n",
    "        if isinstance(file_acc, str):\n",
    "            write_accuracies(file_acc, depth[i], accuracies, var_weight, var_bias)\n",
    "\n",
    "        # optionally write final weights, biases in hdf5 format:\n",
    "        #if write_params and isinstance(file_params, str):\n",
    "        #    write_parameters('e{}-'.format(epochs-1) + file_params, model[i], depth[i], var_weight, var_bias)\n",
    "        \n",
    "    print('\\nTraining complete.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need functions to write and read the data created by the `train_models` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file of accuracies:\n",
    "def write_accuracies(file_name, depth, accuracies, var_weight, var_bias):\n",
    "    with h5py.File(PATH_TO_DATA + re.sub('\\.(.*)$','',file_name) + '-{}.hdf5'.format(depth), 'w') as file:\n",
    "        file.create_dataset('var_weight', data=var_weight)\n",
    "        file.create_dataset('var_bias', data=var_bias)\n",
    "        file.create_dataset('depth', data=depth) \n",
    "        file.create_dataset('accuracies', data=accuracies)\n",
    "\n",
    "        \n",
    "# read file of accuracies, return dataset as dictionary:\n",
    "def read_accuracies(file_name):\n",
    "    with h5py.File(PATH_TO_DATA + file_name, 'r') as file:\n",
    "        # cast elements as np.array, else returns closed file datasets:\n",
    "        acc_dict = {key : np.array(file[key]) for key in file.keys()}  \n",
    "        \n",
    "    return acc_dict\n",
    "\n",
    "\n",
    "# write file of inputs/outputs (n.b., create_dataset tries to turn the data into\n",
    "#   a numpy array, which fails for a list of unevenly-sized tensors; must first\n",
    "#   pre-process and create one dataset for each layer, combining all batches):\n",
    "def write_hooks(file_name, depth, inputs, outputs):\n",
    "    # group data from all batches by layer, by constructing dictionaries of layers;\n",
    "    # keys = layer number, elements = list of batches of inputs/outputs for that layer:\n",
    "    layers_in, layers_out = {i : [] for i in range(depth)}, {i : [] for i in range(depth)}\n",
    "    for key in layers_in.keys():  # same key list for both dicts\n",
    "        [layers_in[key].append(batch) for batch in inputs[key::depth]]\n",
    "        #[layers_out[key].append(batch) for batch in outputs[key::depth]]  # optional/unused\n",
    "       \n",
    "    # concatenate each list of tensors (dict element) into a single tensor (to enable conversion to numpy array):\n",
    "    for key in layers_in.keys():\n",
    "        layers_in[key] = torch.cat(layers_in[key])\n",
    "        #layers_out[key] = torch.cat(layers_out[key])\n",
    "\n",
    "    # write each layer as a dataset:\n",
    "    with h5py.File(PATH_TO_DATA + re.sub('\\.(.*)$','',file_name) + '-{}.hdf5'.format(depth), 'w') as file:\n",
    "        file.create_dataset('var_weight', data=var_weight)\n",
    "        file.create_dataset('var_bias', data=var_bias)\n",
    "        file.create_dataset('depth', data=depth)\n",
    "        for key in layers_in.keys():\n",
    "            # encode whether input or output in key; elements = all inputs/outputs for that layer:\n",
    "            file.create_dataset('in-{}'.format(key), data=layers_in[key]) \n",
    "            #file.create_dataset('out-{}'.format(key), data=layers_out[key])\n",
    "\n",
    "            \n",
    "# read file of inputs/outputs, return dataset as dictionary:\n",
    "def read_hooks(file_name):    \n",
    "    with h5py.File(PATH_TO_DATA + file_name, 'r') as file:\n",
    "        # cast elements as np.array, else returns closed file datasets:\n",
    "        hook_dict = {key : np.array(file[key]) for key in file.keys()}\n",
    "    \n",
    "    return hook_dict\n",
    "\n",
    "\n",
    "# Write weights and biases for entire network in hdf5 format.\n",
    "# Note that last three parameters (depth, var_weight, var_bias)\n",
    "#   are just meta-data, to aid in identifying run upon reading file.\n",
    "def write_parameters(file_name, model, depth, var_weight, var_bias):\n",
    "    with h5py.File(PATH_TO_DATA + re.sub('\\.(.*)$','',file_name) + '-{}.hdf5'.format(depth), 'w') as file:\n",
    "        file.create_dataset('var_weight', data=var_weight)\n",
    "        file.create_dataset('var_bias', data=var_bias)\n",
    "        file.create_dataset('depth', data=depth) \n",
    "        \n",
    "        for key in model.state_dict():\n",
    "            # get correct layer index (instead of x2):\n",
    "            layer_num = int(int(re.findall(r'\\d+', key)[0])/2)\n",
    "                \n",
    "            # write layer's weights/biases as dictionary entry:\n",
    "            if key.endswith('weights'):\n",
    "                file.create_dataset('W{}'.format(layer_num), data=model.state_dict()[key].numpy())     \n",
    "            elif key.endswith('bias'):\n",
    "                file.create_dataset('B{}'.format(layer_num), data=model.state_dict()[key].numpy())   \n",
    "\n",
    "                \n",
    "# read file of weights, biases; return as dictionary:             \n",
    "def read_parameters(file_name):\n",
    "    with h5py.File(PATH_TO_DATA + file_name, 'r') as file:\n",
    "        # cast elements as np.array, else returns closed file datasets:\n",
    "        for key in file.keys():\n",
    "            para_dict = {key : np.array(file[key]) for key in file.keys()}  \n",
    "        \n",
    "    return para_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate datasets (training/testing)\n",
    "Now, let's train some models! First, set the hyperparameters and whatnot used by all models we wish to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters:\n",
    "rate = 0.005\n",
    "epochs = 5\n",
    "momentum = 0.8\n",
    "batch_size = 64\n",
    "\n",
    "# load training & validation data into DataLoaders:\n",
    "train_dl, valid_dl = get_data(x_train, y_train, x_valid, y_valid, batch_size)\n",
    "\n",
    "# set loss function:\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the following cell sequentially trains three models of depth 10, 20, and 30, all with fixed $\\sigma_w^2=2.0$ and $\\sigma_b^2=0.05$.\n",
    "\n",
    "**Note on file name conventions**: when writing data, the length of each run is appended to the given filenames, e.g., passing \"accuracies-20.hdf5\" will result in \"accuracies-20-10.hdf5\", \"accuracies-20-20.hdf5\", and \"accuracies-20-30.hdf5\". The \"-20\" (as in $\\sigma_w^2=2.0$) is my naming convention for keeping runs with different variances straight (though the relevant data to identify them is also written internally). When writing hooks or parameters, then -- for the present example with 5 epochs -- an \"e0-\" and \"e4-\" (\"e\" as in \"epoch\") will be prepended to the hook/parameter filenames, to distinguish pre- vs. post-training results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variances for Gaussian parameter initialization:\n",
    "var_weight = 2.0\n",
    "var_bias = 0.05\n",
    "\n",
    "train_models(10,31,10, file_acc='accuracies-20.hdf5',\n",
    "             write_params=True, file_params='parameters-20.hdf5',\n",
    "             hooks=True, file_hook='hooks-20.hdf5', save_model=True, file_model='models-20.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "To perform a more systematic search of parameter space as in fig. 6 of the companion paper, the following cell trains a list of networks with depths $L\\in\\{10,13,16,\\ldots,67,70\\}$ with $\\sigma_w^2\\in\\{1.00, 1.05, 1.10, \\ldots, 2.95,3.00\\}$, and fixed $\\sigma_b^2=0.05$. Here we only care about the accuracies, so we'll run with `hooks=False` for speed, and not bother writing the parameters or the models themselves either (to save memory). I don't recommend doing this on a standard desktop, unless you are inordinately patient.\n",
    "\n",
    "The same naming convention as in the previous example is used when writing the accuracies here: at $\\sigma_w^2=1.00$, we'll have \"acc-100-10.hdf5\", \"acc-100-13.hdf5\", and so on; at $\\sigma_w^2=1.05$, we'll have \"acc-105-10.hdf5\", \"acc-105-13.hdf5\", and so on; etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_bias = 0.05\n",
    "\n",
    "# iterate over a range of var_w from 1.0 to 3.0, in steps of 0.05:\n",
    "for i in range(100,301,5):\n",
    "    var_weight = i/100\n",
    "    file_acc = 'acc-{}.hdf5'.format(i)  # base filename\n",
    "    \n",
    "    print('Training models with variance {}...\\n'.format(var_weight))\n",
    "\n",
    "    train_models(10,73,3, file_acc=file_acc, write_params=False, file_params='dummy_para_name.hdf5', \n",
    "                 hooks=False, file_hook='dummy_hook_name.hdf5', save_model=False, file_model='dummy_model_name.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
